{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chemical-pride",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class _PyTorchGradient(Explainer):\n",
    "\n",
    "    def __init__(self, model, data, batch_size=50, local_smoothing=0):\n",
    "\n",
    "        # check if we have multiple inputs\n",
    "        self.multi_input = False\n",
    "        if type(data) == list:\n",
    "            self.multi_input = True\n",
    "        if type(data) != list:\n",
    "            data = [data]\n",
    "\n",
    "        # for consistency, the method signature calls for data as the model input.\n",
    "        # However, within this class, self.model_inputs is the input (i.e. the data passed by the user)\n",
    "        # and self.data is the background data for the layer we want to assign importances to. If this layer is\n",
    "        # the input, then self.data = self.model_inputs\n",
    "        self.model_inputs = data\n",
    "        self.batch_size = batch_size\n",
    "        self.local_smoothing = local_smoothing\n",
    "\n",
    "        self.layer = None\n",
    "        self.input_handle = None\n",
    "        self.interim = False\n",
    "        if type(model) == tuple:\n",
    "            self.interim = True\n",
    "            model, layer = model\n",
    "            model = model.eval()\n",
    "            self.add_handles(layer)\n",
    "            self.layer = layer\n",
    "\n",
    "            # now, if we are taking an interim layer, the 'data' is going to be the input\n",
    "            # of the interim layer; we will capture this using a forward hook\n",
    "            with torch.no_grad():\n",
    "                _ = model(*data)\n",
    "                interim_inputs = self.layer.target_input\n",
    "                if type(interim_inputs) is tuple:\n",
    "                    # this should always be true, but just to be safe\n",
    "                    self.data = [i.clone().detach() for i in interim_inputs]\n",
    "                else:\n",
    "                    self.data = [interim_inputs.clone().detach()]\n",
    "        else:\n",
    "            self.data = data\n",
    "        self.model = model.eval()\n",
    "\n",
    "        multi_output = False\n",
    "        outputs = self.model(*self.model_inputs)\n",
    "        if len(outputs.shape) > 1 and outputs.shape[1] > 1:\n",
    "            multi_output = True\n",
    "        self.multi_output = multi_output\n",
    "\n",
    "        if not self.multi_output:\n",
    "            self.gradients = [None]\n",
    "        else:\n",
    "            self.gradients = [None for i in range(outputs.shape[1])]\n",
    "\n",
    "    def gradient(self, idx, inputs):\n",
    "        self.model.zero_grad()\n",
    "        X = [x.requires_grad_() for x in inputs]\n",
    "        outputs = self.model(*X)\n",
    "        selected = [val for val in outputs[:, idx]]\n",
    "        if self.input_handle is not None:\n",
    "            interim_inputs = self.layer.target_input\n",
    "            grads = [torch.autograd.grad(selected, input,\n",
    "                                         retain_graph=True if idx + 1 < len(interim_inputs) else None)[0].cpu().numpy()\n",
    "                     for idx, input in enumerate(interim_inputs)]\n",
    "            del self.layer.target_input\n",
    "        else:\n",
    "            grads = [torch.autograd.grad(selected, x,\n",
    "                                         retain_graph=True if idx + 1 < len(X) else None)[0].cpu().numpy()\n",
    "                     for idx, x in enumerate(X)]\n",
    "        return grads\n",
    "\n",
    "    @staticmethod\n",
    "    def get_interim_input(self, input, output):\n",
    "        try:\n",
    "            del self.target_input\n",
    "        except AttributeError:\n",
    "            pass\n",
    "        setattr(self, 'target_input', input)\n",
    "\n",
    "    def add_handles(self, layer):\n",
    "        input_handle = layer.register_forward_hook(self.get_interim_input)\n",
    "        self.input_handle = input_handle\n",
    "\n",
    "    def shap_values(self, X, nsamples=200, ranked_outputs=None, output_rank_order=\"max\", rseed=None, return_variances=False):\n",
    "\n",
    "        # X ~ self.model_input\n",
    "        # X_data ~ self.data\n",
    "\n",
    "        # check if we have multiple inputs\n",
    "        if not self.multi_input:\n",
    "            assert type(X) != list, \"Expected a single tensor model input!\"\n",
    "            X = [X]\n",
    "        else:\n",
    "            assert type(X) == list, \"Expected a list of model inputs!\"\n",
    "\n",
    "        if ranked_outputs is not None and self.multi_output:\n",
    "            with torch.no_grad():\n",
    "                model_output_values = self.model(*X)\n",
    "            # rank and determine the model outputs that we will explain\n",
    "            if output_rank_order == \"max\":\n",
    "                _, model_output_ranks = torch.sort(model_output_values, descending=True)\n",
    "            elif output_rank_order == \"min\":\n",
    "                _, model_output_ranks = torch.sort(model_output_values, descending=False)\n",
    "            elif output_rank_order == \"max_abs\":\n",
    "                _, model_output_ranks = torch.sort(torch.abs(model_output_values), descending=True)\n",
    "            else:\n",
    "                assert False, \"output_rank_order must be max, min, or max_abs!\"\n",
    "            model_output_ranks = model_output_ranks[:, :ranked_outputs]\n",
    "        else:\n",
    "            model_output_ranks = (torch.ones((X[0].shape[0], len(self.gradients))).int() *\n",
    "                                  torch.arange(0, len(self.gradients)).int())\n",
    "\n",
    "        # if a cleanup happened, we need to add the handles back\n",
    "        # this allows shap_values to be called multiple times, but the model to be\n",
    "        # 'clean' at the end of each run for other uses\n",
    "        if self.input_handle is None and self.interim is True:\n",
    "            self.add_handles(self.layer)\n",
    "\n",
    "        # compute the attributions\n",
    "        X_batches = X[0].shape[0]\n",
    "        output_phis = []\n",
    "        output_phi_vars = []\n",
    "        # samples_input = input to the model\n",
    "        # samples_delta = (x - x') for the input being explained - may be an interim input\n",
    "        samples_input = [torch.zeros((nsamples,) + X[l].shape[1:], device=X[l].device) for l in range(len(X))]\n",
    "        samples_delta = [np.zeros((nsamples, ) + self.data[l].shape[1:]) for l in range(len(self.data))]\n",
    "\n",
    "        # use random seed if no argument given\n",
    "        if rseed is None:\n",
    "            rseed = np.random.randint(0, 1e6)\n",
    "\n",
    "        for i in range(model_output_ranks.shape[1]):\n",
    "            np.random.seed(rseed)  # so we get the same noise patterns for each output class\n",
    "            phis = []\n",
    "            phi_vars = []\n",
    "            for k in range(len(self.data)):\n",
    "                # for each of the inputs being explained - may be an interim input\n",
    "                phis.append(np.zeros((X_batches,) + self.data[k].shape[1:]))\n",
    "                phi_vars.append(np.zeros((X_batches, ) + self.data[k].shape[1:]))\n",
    "            for j in range(X[0].shape[0]):\n",
    "                # fill in the samples arrays\n",
    "                for k in range(nsamples):\n",
    "                    rind = np.random.choice(self.data[0].shape[0])\n",
    "                    t = np.random.uniform()\n",
    "                    for l in range(len(X)):\n",
    "                        if self.local_smoothing > 0:\n",
    "                            # local smoothing is added to the base input, unlike in the TF gradient explainer\n",
    "                            x = X[l][j].clone().detach() + torch.empty(X[l][j].shape, device=X[l].device).normal_() \\\n",
    "                                * self.local_smoothing\n",
    "                        else:\n",
    "                            x = X[l][j].clone().detach()\n",
    "                        samples_input[l][k] = (t * x + (1 - t) * (self.model_inputs[l][rind]).clone().detach()).\\\n",
    "                            clone().detach()\n",
    "                        if self.input_handle is None:\n",
    "                            samples_delta[l][k] = (x - (self.data[l][rind]).clone().detach()).cpu().numpy()\n",
    "\n",
    "                    if self.interim is True:\n",
    "                        with torch.no_grad():\n",
    "                            _ = self.model(*[samples_input[l][k].unsqueeze(0) for l in range(len(X))])\n",
    "                            interim_inputs = self.layer.target_input\n",
    "                            del self.layer.target_input\n",
    "                            if type(interim_inputs) is tuple:\n",
    "                                if type(interim_inputs) is tuple:\n",
    "                                    # this should always be true, but just to be safe\n",
    "                                    for l in range(len(interim_inputs)):\n",
    "                                        samples_delta[l][k] = interim_inputs[l].cpu().numpy()\n",
    "                                else:\n",
    "                                    samples_delta[0][k] = interim_inputs.cpu().numpy()\n",
    "\n",
    "                # compute the gradients at all the sample points\n",
    "                find = model_output_ranks[j, i]\n",
    "                grads = []\n",
    "                for b in range(0, nsamples, self.batch_size):\n",
    "                    batch = [samples_input[l][b:min(b+self.batch_size,nsamples)].clone().detach() for l in range(len(X))]\n",
    "                    grads.append(self.gradient(find, batch))\n",
    "                grad = [np.concatenate([g[l] for g in grads], 0) for l in range(len(self.data))]\n",
    "                # assign the attributions to the right part of the output arrays\n",
    "                for l in range(len(self.data)):\n",
    "                    samples = grad[l] * samples_delta[l]\n",
    "                    phis[l][j] = samples.mean(0)\n",
    "                    phi_vars[l][j] = samples.var(0) / np.sqrt(samples.shape[0]) # estimate variance of means\n",
    "\n",
    "            output_phis.append(phis[0] if len(self.data) == 1 else phis)\n",
    "            output_phi_vars.append(phi_vars[0] if not self.multi_input else phi_vars)\n",
    "        # cleanup: remove the handles, if they were added\n",
    "        if self.input_handle is not None:\n",
    "            self.input_handle.remove()\n",
    "            self.input_handle = None\n",
    "            # note: the target input attribute is deleted in the loop\n",
    "\n",
    "        if not self.multi_output:\n",
    "            if return_variances:\n",
    "                return output_phis[0], output_phi_vars[0]\n",
    "            else:\n",
    "                return output_phis[0]\n",
    "        elif ranked_outputs is not None:\n",
    "            if return_variances:\n",
    "                return output_phis, output_phi_vars, model_output_ranks\n",
    "            else:\n",
    "                return output_phis, model_output_ranks\n",
    "        else:\n",
    "            if return_variances:\n",
    "                return output_phis, output_phi_vars\n",
    "            else:\n",
    "                return output_phis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hungry-eight",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fleet-probability",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "classified-giant",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "touched-steering",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "metropolitan-spare",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "taken-cement",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bulgarian-munich",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "genetic-willow",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "round-holiday",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "import functools\n",
    "import operator\n",
    "import torch\n",
    "from torch.autograd import grad\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "DEFAULT_DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def gather_nd(params, indices):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        params: Tensor to index\n",
    "        indices: k-dimension tensor of integers. \n",
    "    Returns:\n",
    "        output: 1-dimensional tensor of elements of ``params``, where\n",
    "            output[i] = params[i][indices[i]]\n",
    "\n",
    "            params   indices   output\n",
    "\n",
    "            1 2       1 1       4\n",
    "            3 4       2 0 ----> 5\n",
    "            5 6       0 0       1\n",
    "    \"\"\"\n",
    "    max_value = functools.reduce(operator.mul, list(params.size())) - 1\n",
    "    indices = indices.t().long()\n",
    "    ndim = indices.size(0)\n",
    "    idx = torch.zeros_like(indices[0]).long()\n",
    "    m = 1\n",
    "\n",
    "    for i in range(ndim)[::-1]:\n",
    "        idx += indices[i]*m\n",
    "        m *= params.size(i)\n",
    "\n",
    "    idx[idx < 0] = 0\n",
    "    idx[idx > max_value] = 0\n",
    "    return torch.take(params, idx)\n",
    "    \n",
    "\n",
    "class AttributionPriorExplainer(object):\n",
    "    def __init__(self, background_dataset, batch_size, random_alpha=True,k=1,scale_by_inputs=True):\n",
    "        self.random_alpha = random_alpha\n",
    "        self.k = k\n",
    "        self.scale_by_inputs = scale_by_inputs\n",
    "        self.batch_size = batch_size\n",
    "        self.ref_set = background_dataset\n",
    "        self.ref_sampler = DataLoader(\n",
    "                dataset=background_dataset, \n",
    "                batch_size=batch_size*k, \n",
    "                shuffle=True, \n",
    "                drop_last=True)\n",
    "        return\n",
    "\n",
    "    def _get_ref_batch(self,k=None):\n",
    "        return next(iter(self.ref_sampler))[0].float()\n",
    "    \n",
    "    def _get_samples_input(self, input_tensor, reference_tensor):\n",
    "        '''\n",
    "        calculate interpolation points\n",
    "        Args:\n",
    "            input_tensor: Tensor of shape (batch, ...), where ... indicates\n",
    "                          the input dimensions. \n",
    "            reference_tensor: A tensor of shape (batch, k, ...) where ... \n",
    "                indicates dimensions, and k represents the number of background \n",
    "                reference samples to draw per input in the batch.\n",
    "        Returns: \n",
    "            samples_input: A tensor of shape (batch, k, ...) with the \n",
    "                interpolated points between input and ref.\n",
    "        '''\n",
    "        input_dims = list(input_tensor.size())[1:]\n",
    "        num_input_dims = len(input_dims)\n",
    "            \n",
    "        batch_size = reference_tensor.size()[0]\n",
    "        k_ = reference_tensor.size()[1]\n",
    "\n",
    "        # Grab a [batch_size, k]-sized interpolation sample\n",
    "        if self.random_alpha:\n",
    "            t_tensor = torch.FloatTensor(batch_size, k_).uniform_(0,1).to(DEFAULT_DEVICE)\n",
    "        else:\n",
    "            if k_==1:\n",
    "                t_tensor = torch.cat([torch.Tensor([1.0]) for i in range(batch_size)]).to(DEFAULT_DEVICE)\n",
    "            else:\n",
    "                t_tensor = torch.cat([torch.linspace(0,1,k_) for i in range(batch_size)]).to(DEFAULT_DEVICE)\n",
    "\n",
    "        shape = [batch_size, k_] + [1] * num_input_dims\n",
    "        interp_coef = t_tensor.view(*shape)\n",
    "\n",
    "        # Evaluate the end points\n",
    "        end_point_ref = (1.0 - interp_coef) * reference_tensor\n",
    "\n",
    "        input_expand_mult = input_tensor.unsqueeze(1)\n",
    "        end_point_input = interp_coef * input_expand_mult\n",
    "        \n",
    "        # A fine Affine Combine\n",
    "        samples_input = end_point_input + end_point_ref\n",
    "        return samples_input\n",
    "    \n",
    "    def _get_samples_delta(self, input_tensor, reference_tensor):\n",
    "        input_expand_mult = input_tensor.unsqueeze(1)\n",
    "        sd = input_expand_mult - reference_tensor\n",
    "        return sd\n",
    "    \n",
    "    def _get_grads(self, samples_input, model, sparse_labels=None):\n",
    "        samples_input.requires_grad = True\n",
    "\n",
    "        grad_tensor = torch.zeros(samples_input.shape).float().to(DEFAULT_DEVICE)\n",
    "\n",
    "        \n",
    "        for i in range(self.k):\n",
    "            particular_slice = samples_input[:,i]\n",
    "            batch_output = model(particular_slice)\n",
    "            # should check that users pass in sparse labels\n",
    "            # Only look at the user-specified label\n",
    "            if batch_output.size(1) > 1:\n",
    "                sample_indices = torch.arange(0,batch_output.size(0)).to(DEFAULT_DEVICE)\n",
    "                indices_tensor = torch.cat([\n",
    "                        sample_indices.unsqueeze(1), \n",
    "                        sparse_labels.unsqueeze(1)], dim=1)\n",
    "                batch_output = gather_nd(batch_output, indices_tensor)\n",
    "\n",
    "            model_grads = grad(\n",
    "                    outputs=batch_output,\n",
    "                    inputs=particular_slice,\n",
    "                    grad_outputs=torch.ones_like(batch_output).to(DEFAULT_DEVICE),\n",
    "                    create_graph=True)\n",
    "            grad_tensor[:,i,:] = model_grads[0]\n",
    "        return grad_tensor\n",
    "           \n",
    "    def shap_values(self, model, input_tensor, sparse_labels=None):\n",
    "        \"\"\"\n",
    "        Calculate expected gradients approximation of Shapley values for the \n",
    "        sample ``input_tensor``.\n",
    "\n",
    "        Args:\n",
    "            model (torch.nn.Module): Pytorch neural network model for which the\n",
    "                output should be explained.\n",
    "            input_tensor (torch.Tensor): Pytorch tensor representing the input\n",
    "                to be explained.\n",
    "            sparse_labels (optional, default=None): \n",
    "        \"\"\"\n",
    "        reference_tensor = self._get_ref_batch()\n",
    "        shape = reference_tensor.shape\n",
    "        reference_tensor = reference_tensor.view(\n",
    "                self.batch_size, \n",
    "                self.k, \n",
    "                *(shape[1:])).to(DEFAULT_DEVICE)\n",
    "        samples_input = self._get_samples_input(input_tensor, reference_tensor)\n",
    "        samples_delta = self._get_samples_delta(input_tensor, reference_tensor)\n",
    "        grad_tensor = self._get_grads(samples_input, model, sparse_labels)\n",
    "        mult_grads = samples_delta * grad_tensor if self.scale_by_inputs else grad_tensor\n",
    "        expected_grads = mult_grads.mean(1)\n",
    "        return expected_grads\n",
    "\n",
    "class VariableBatchExplainer(AttributionPriorExplainer):\n",
    "    \"\"\"\n",
    "    Subclasses AttributionPriorExplainer to avoid pre-specified batch size. Will adapt batch\n",
    "    size based on shape of input tensor.\n",
    "    \"\"\"\n",
    "    def __init__(self, background_dataset, random_alpha=True,scale_by_inputs=True):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "        background_dataset: PyTorch dataset - may not work with iterable-type (vs map-type) datasets\n",
    "        random_alpha: boolean - Whether references should be interpolated randomly (True, corresponds\n",
    "            to Expected Gradients) or on a uniform grid (False - corresponds to Integrated Gradients)\n",
    "        \"\"\"\n",
    "        self.random_alpha = random_alpha\n",
    "        self.k = None\n",
    "        self.scale_by_inputs=scale_by_inputs\n",
    "        self.ref_set = background_dataset\n",
    "        self.ref_sampler = DataLoader(\n",
    "                dataset=background_dataset, \n",
    "                batch_size=1, \n",
    "                shuffle=True, \n",
    "                drop_last=True)\n",
    "        self.refs_needed = -1\n",
    "        return\n",
    "\n",
    "    def _get_ref_batch(self,refs_needed=None):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "        refs_needed: int - number of references to provide\n",
    "        \"\"\"\n",
    "        if refs_needed!=self.refs_needed:\n",
    "            self.ref_sampler = DataLoader(\n",
    "                dataset=self.ref_set, \n",
    "                batch_size=refs_needed, \n",
    "                shuffle=True, \n",
    "                drop_last=True)\n",
    "            self.refs_needed = refs_needed\n",
    "        return next(iter(self.ref_sampler))[0].float()\n",
    "           \n",
    "    def shap_values(self, model, input_tensor, sparse_labels=None,k=1):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "        base_model: PyTorch network\n",
    "        input_tensor: PyTorch tensor to get attributions for, as in normal torch.nn.Module API\n",
    "        sparse_labels:  np.array of sparse integer labels, i.e. 0-9 for MNIST. Used if you only\n",
    "            want to explain the prediction for the true class per sample.\n",
    "        k: int - Number of references to use default for explanations. As low as 1 for training.\n",
    "            100-200 for reliable explanations. \n",
    "        \"\"\"\n",
    "        self.k = k\n",
    "        n_input = input_tensor.shape[0]\n",
    "        refs_needed = n_input*self.k\n",
    "        # This is a reasonable check but prevents compatibility with non-Map datasets\n",
    "        assert refs_needed<=len(self.ref_set), \"Can't have more samples*references than there are reference points!\"\n",
    "        reference_tensor = self._get_ref_batch(refs_needed)\n",
    "        shape = reference_tensor.shape\n",
    "        reference_tensor = reference_tensor.view(\n",
    "                n_input, \n",
    "                self.k,\n",
    "                *(shape[1:])).to(DEFAULT_DEVICE)\n",
    "        samples_input = self._get_samples_input(input_tensor, reference_tensor)\n",
    "        samples_delta = self._get_samples_delta(input_tensor, reference_tensor)\n",
    "        grad_tensor = self._get_grads(samples_input, model, sparse_labels)\n",
    "        mult_grads = samples_delta * grad_tensor if self.scale_by_inputs else grad_tensor\n",
    "        expected_grads = mult_grads.mean(1)\n",
    "\n",
    "        return expected_grads\n",
    "\n",
    "class ExpectedGradientsModel(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Wraps a PyTorch model (one that implements torch.nn.Module) so that model(x)\n",
    "    produces SHAP values as well as predictions (controllable by 'shap_values'\n",
    "    flag.\n",
    "    \"\"\"\n",
    "    def __init__(self,base_model,refset,k=1,random_alpha=True,scale_by_inputs=True):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "        base_model: PyTorch network that subclasses torch.nn.Module\n",
    "        refset: PyTorch dataset - may not work with iterable-type (vs map-type) datasets\n",
    "        k: int - Number of references to use by default for explanations. As low as 1 for training.\n",
    "            100-200 for reliable explanations. \n",
    "        \"\"\"\n",
    "        super(ExpectedGradientsModel,self).__init__()\n",
    "        self.k = k\n",
    "        self.base = base_model\n",
    "        self.refset = refset\n",
    "        self.random_alpha = random_alpha\n",
    "        self.exp = VariableBatchExplainer(self.refset,random_alpha=random_alpha,scale_by_inputs=scale_by_inputs)\n",
    "    def forward(self,x,shap_values=False,sparse_labels=None,k=1):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "        x: PyTorch tensor to predict with, as in normal torch.nn.Module API\n",
    "        shap_values:     Binary flag -- whether to produce SHAP values\n",
    "        sparse_labels:  np.array of sparse integer labels, i.e. 0-9 for MNIST. Used if you only\n",
    "            want to explain the prediction for the true class per sample.\n",
    "        k: int - Number of references to use default for explanations. As low as 1 for training.\n",
    "            100-200 for reliable explanations. \n",
    "        \"\"\"\n",
    "        output = self.base(x)\n",
    "        if not shap_values: return output\n",
    "        else: shaps = self.exp.shap_values(self.base,x,sparse_labels=sparse_labels,k=k)\n",
    "        return output, shaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sorted-questionnaire",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "japanese-stuff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MarcoPolo",
   "language": "python",
   "name": "marcopolo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
