{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "viral-formula",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code gratuitously stolen from: \n",
    "# https://github.com/hwalsuklee/tensorflow-mnist-cnn\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy\n",
    "\n",
    "import tensorflow as tf  \n",
    "import numpy as np\n",
    "import tensorflow.contrib.slim as slim\n",
    "import datetime \n",
    "\n",
    "import mnist_model\n",
    "import mnist_reader\n",
    "\n",
    "from attributionpriors.ops import AttributionPriorExplainer\n",
    "\n",
    "FLAGS = tf.app.flags.FLAGS\n",
    "\n",
    "tf.app.flags.DEFINE_integer('batch_size', 50,\n",
    "                       \"\"\"Batch size for training.\"\"\")\n",
    "tf.app.flags.DEFINE_integer('epochs', 60,\n",
    "                       \"\"\"Number of epochs to train for.\"\"\")\n",
    "tf.app.flags.DEFINE_float('lamb', 0.0,\n",
    "                         \"\"\"Penalty between training loss (0.0) and regularization loss (1.0)\"\"\")\n",
    "tf.app.flags.DEFINE_float('eta', 0.0001,\n",
    "                         \"\"\"initial learning rate\"\"\")\n",
    "tf.app.flags.DEFINE_float('keep_prob', 0.5,\n",
    "                         'Dropout keep probability')\n",
    "\n",
    "MODEL_DIRECTORY = \"models/{}/model.ckpt\"\n",
    "TRAIN_LOGS_DIRECTORY = \"logs/{}/train\"\n",
    "EVAL_LOGS_DIRECTORY  = \"logs/{}/vald\"\n",
    "\n",
    "# Params for Train\n",
    "validation_step = 500\n",
    "\n",
    "# Params for test\n",
    "TEST_BATCH_SIZE = 50\n",
    "\n",
    "#Data params\n",
    "IMAGE_SIZE = 28\n",
    "NUM_CHANNELS = 1\n",
    "PIXEL_DEPTH = 255\n",
    "NUM_LABELS = 10\n",
    "VALIDATION_SIZE = 50\n",
    "\n",
    "def batch_standardize(frames):\n",
    "        return tf.map_fn(lambda frame: tf.image.per_image_standardization(frame), frames)\n",
    "\n",
    "def inputs(train_batch, train_epochs,\n",
    "           vald_batch=50,\n",
    "           test_batch=50):\n",
    "    training_iterator   = mnist_reader.inputs('train', train_batch, train_epochs)\n",
    "    validation_iterator = mnist_reader.inputs('vald',  vald_batch, None)\n",
    "    test_iterator       = mnist_reader.inputs('test',  test_batch, None)\n",
    "\n",
    "    handle = tf.placeholder(tf.string, shape=[])\n",
    "    iterator = tf.data.Iterator.from_string_handle(\n",
    "                handle, (tf.float32, tf.int32), ((None, 28, 28, 1), (None, )))\n",
    "    x, y_ = iterator.get_next()\n",
    "    return training_iterator, validation_iterator, test_iterator, handle, x, y_\n",
    "\n",
    "def get_model(cond_input_op):\n",
    "    train_pl = tf.placeholder_with_default(False, shape=(), name='train_pl')\n",
    "    y = mnist_model.model(cond_input_op, train_pl)\n",
    "    return y, train_pl\n",
    "\n",
    "def get_learning_rate(init_learning_rate, train_batch):\n",
    "    global_step = tf.train.get_or_create_global_step()\n",
    "    learning_rate = tf.train.exponential_decay(init_learning_rate, \n",
    "                                                global_step * train_batch, \n",
    "                                                60000,\n",
    "                                                0.95,\n",
    "                                                staircase=True)\n",
    "    return learning_rate\n",
    "\n",
    "def pipeline(use_old_model=False):\n",
    "    # Data iterators\n",
    "    training_iterator, validation_iterator, test_iterator, handle, x, y_ = inputs(FLAGS.batch_size, FLAGS.epochs,\n",
    "                                                                                  VALIDATION_SIZE,\n",
    "                                                                                  TEST_BATCH_SIZE)\n",
    "    \n",
    "    # Get explainer conditional input\n",
    "    explainer = AttributionPriorExplainer()\n",
    "    cond_input_op, train_eg = explainer.input_to_samples_delta(x)\n",
    "    \n",
    "    # Predict\n",
    "    y, train_pl = get_model(cond_input_op)\n",
    "    \n",
    "    # Get explanations\n",
    "    expected_gradients_op = explainer.shap_value_op(y, cond_input_op, sparse_labels_op=y_)\n",
    "    expected_gradients_op = batch_standardize(expected_gradients_op)\n",
    "    \n",
    "    learning_rate = get_learning_rate(FLAGS.eta, FLAGS.batch_size)\n",
    "    global_step = tf.train.get_or_create_global_step()\n",
    "    \n",
    "    loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y_, logits=y))\n",
    "    train_step = tf.train.AdamOptimizer(learning_rate).minimize((1.0 - FLAGS.lamb) * loss, global_step=global_step)\n",
    "\n",
    "    eg_loss  = tf.reduce_mean(tf.image.total_variation(expected_gradients_op, name='variation'))\n",
    "    eg_train = tf.train.AdamOptimizer(learning_rate).minimize(FLAGS.lamb * eg_loss, global_step=global_step)\n",
    "    \n",
    "    y_pred = tf.argmax(y, 1)\n",
    "    accuracy_op, accuracy_update_op = tf.metrics.accuracy(y_pred, y_)\n",
    "    reset_metrics_op = tf.variables_initializer(tf.get_collection(tf.GraphKeys.METRIC_VARIABLES))\n",
    "    \n",
    "    return cond_input_op, x, y, y_, train_pl, loss, train_step, eg_loss, eg_train, train_eg, accuracy_op, accuracy_update_op, reset_metrics_op, training_iterator, validation_iterator, test_iterator, handle\n",
    "\n",
    "def train(save_dir, sess, cond_input_op, y, train_pl, loss, train_step, eg_loss, eg_train, \\\n",
    "          train_eg, accuracy_op, accuracy_update_op, reset_metrics_op, training_iterator, \\\n",
    "          validation_iterator, test_iterator, handle):\n",
    "    \n",
    "    \n",
    "    init = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\n",
    "    sess.run(init)\n",
    "\n",
    "    training_handle   = sess.run(training_iterator.string_handle())\n",
    "    validation_handle = sess.run(validation_iterator.string_handle())\n",
    "    test_handle       = sess.run(test_iterator.string_handle())\n",
    "\n",
    "    max_acc = 0.\n",
    "\n",
    "    sess.run(training_iterator.initializer)\n",
    "    global_step = tf.train.get_or_create_global_step()\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    validation_accuracies = []\n",
    "    validation_total_variances = []\n",
    "    while True:\n",
    "        try:\n",
    "            _, _, i = sess.run([train_step, accuracy_update_op, global_step] , feed_dict={handle: training_handle,\n",
    "                                                                                          train_pl: True})\n",
    "            if FLAGS.lamb > 0.0:\n",
    "                _, train_var_loss = sess.run([eg_train, eg_loss], feed_dict={handle: training_handle, \n",
    "                                                                             train_eg: True})\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            break\n",
    "\n",
    "        if i % validation_step == 0:\n",
    "            sess.run(reset_metrics_op)\n",
    "            sess.run(validation_iterator.initializer)\n",
    "            while True:\n",
    "                try:\n",
    "                    sess.run(accuracy_update_op, feed_dict={handle: validation_handle})\n",
    "                except tf.errors.OutOfRangeError:\n",
    "                    break\n",
    "\n",
    "            sess.run(validation_iterator.initializer)\n",
    "            vald_var_loss = sess.run(eg_loss, feed_dict={handle: validation_handle, train_eg: True})\n",
    "            validation_accuracy = sess.run(accuracy_op, feed_dict={handle: validation_handle})\n",
    "            \n",
    "            validation_total_variances.append(vald_var_loss)\n",
    "            validation_accuracies.append(validation_accuracy)\n",
    "\n",
    "            print('Iteration: {}, validation accuracy: {:.6f}, e-variation vald (batch): {:.6f}'.format(i, validation_accuracy, vald_var_loss), end='\\r')\n",
    "            sess.run(reset_metrics_op)\n",
    "\n",
    "        if validation_accuracy > max_acc:\n",
    "            max_acc = validation_accuracy\n",
    "            save_path = saver.save(sess, save_dir)\n",
    "    \n",
    "    # Restore variables from disk\n",
    "    saver.restore(sess, save_dir)\n",
    "\n",
    "    # Loop over all batches\n",
    "    sess.run(reset_metrics_op)\n",
    "    sess.run(test_iterator.initializer)\n",
    "    while True:\n",
    "        try:\n",
    "            sess.run(accuracy_update_op, feed_dict={handle: test_handle, train_pl: False})\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            break\n",
    "    test_accuracy = sess.run(accuracy_op)\n",
    "    print('Test accuracy: {:.6f}'.format(test_accuracy))\n",
    "    return validation_total_variances, validation_accuracies, test_accuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MarcoPolo",
   "language": "python",
   "name": "marcopolo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
